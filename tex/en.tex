\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}

\begin{document}

\title{EFFECTIVE KOSOZERMITE PRECONDITIONERS FOR SOLVING HERRING PROBLEMS}
\author{Martynova T.S., Mumyatova G.B., Shabas I.N.}
\date{GOU VPO \textit{Institute of Mathematics, Mechanics and Computer Science named after Vorovich I.I.}, Rostov-on-Don}
\maketitle

\section*{Introduction}

The theory of iterative methods for systems with linear algebraic equations (SLA) is extensive and sufficiently developed. When solving SLOWS, one of the key points is preconditioning, i.e. the choice of some matrices that significantly affect the rate of convergence of methods.

One of the current trends in the theory of iterative methods is the development for solving a wide class of herring problems. Krylov subspace methods, such as GMRES, are usually used to solve large-dimensional SLOWS, but they can become more complicated when applied to herring problems. Therefore, it is necessary to create good preconditioners in order to improve the convergence rate of these methods.

Consider an iterative solution of a large sparse SLOUGH

\[ Av = b, \quad b \in \mathbb{C}^n, \]

where \(A\in\mathbb{C}^{n\times n}\) is an irregularly positive definite matrix.

Preconditioning means that system (1) is replaced by system \(B^{-1}Av = B^{-1}b\), where the preconditioning matrix \(B\) is close to \(A\). Thus, the system \(Bv = b\) is solved easily. A suitable choice \(B\) can make the condition number of the matrix \(B^{-1}A\) small. Many factors influence the choice of a good precondition. At the same time, this moment is not conditioned by a single theory and requires additional research.

\section{Iterative methods for solving SLOWS with a strongly irregular matrix}

The matrix \(A\) of system (1) is represented as the sum of its diagonal and oblique part:
\[ A = A_0 + A_1, \]
where
\[A_0 = \frac{1}{2}(A + A^*), \quad A_1 = \frac{1}{2}(A - A^*). \] (3)

Positive definiteness of A means that for any \(z\in\mathbb{C}^n\backslash \{0\} \), \( x^*A_0x > 0 \). Let in some matrix norm |||·|||, \( ||| A_0|||\ll|||A_1|||\), i.e. the matrix \(A\) is strongly irregular [1]. This situation arises, for example, when the Navier-Stokes equation is discretized, when the coefficients strongly dominate [2]. In addition, in this paper it is assumed that \(diag(A_1) = 0\). This condition is satisfied automatically if the elements of the matrix \(A\) are valid.

Imagine the oblique part \(A_1\) of the matrix \(A\) as:
\[ A_1 = K_l + K_u, \] (4)

where \(K_l\) and \(K_u\) are strictly lower- and upper-triangular matrices, respectively. Obviously, \(K_l = -K^*_u\).

On the basis of the considered (2)-(4) in [3, 4, 5], classes of triangular (TCM) and semi-alternating triangular (PTTCM) kosozermite (skew-symmetric) iterative methods for solving system (1) are proposed. In this paper, methods related to PTTCM are considered.

The PTTCM method [5]. Let the initial approximation \(v(0)\) and the positive parameters \(h\) and \(\tau\) be given. For \(p = 0,1, ... \) a sufficient sequence of approximations \( \{v(p)\} \) is calculated:

\[ v(p+1) = G(v, \tau)v(p) + \tau B(\omega)^{-1}b, \]
where \(G(v, \tau) = B(\omega)^{-1}(B(\omega) - \tau A) \), \(B(\omega)\in\mathbb{C}^{n\times n}\) is defined as follows:
\[B(\omega) = (B_e + \frac{\omega}{2} K_l)^{-1}(B_e + \frac{\omega}{2}K_u). \]

Here, \(B_e\in \mathbb{C}^{n\times n}\) is a symmetric positive definite matrix.

\[B(\omega) \] is investigated for two parametric methods, for which:
\[B(\omega_1, \omega_2) = (B_e +\omega_1 K_l)^{-1}(B_e+\omega_2 K_u), \](5)

where \(\omega_1\) and \( \omega_2\) are non—negative parameters that are not equal to zero at the same time

% Insert the provided content below
In [7, 8], a two-step coordinate iterative method (DCM) is also proposed, sufficient conditions for the convergence of the method and the choice of optimal iterative parameters are given. Matrix \(B(\omega) \) for DCM has the form
\[ B(\omega) = \left(B_c + \frac{\omega}{2} K_L\right)B_E \left(B_c + \frac{\omega}{2} K_U\right), \]
where \(K_L = K_L + H_0, K_U = K_U - H_0 \). But \(B_c = C_{\times}\) is some sparse matrix, \(B_E=C_{\times}\) is a locally quasi–precisely defined matrix. Obviously, \(K_L = -K_U; A = (K_L + H_0) + (K_U - H_0) = K_L + K_U \). 
In the case when \(H_0 = 0\), DCM is reduced to ITCM, a special choice of the matrix \(H_0\) allows to improve the convergence of the method.
In [9], a generalized sparse triangular GSTS (Generalized Skew-Hermitian Triangular Splitting) method was proposed for the first time for solving basic SLOWS whose block-structured matrices have a positive definite (1, 1) block.
In this paper, the properties of the matrix \(B(\omega)\) (6) are investigated using the SLOUGH matrix (1) as a preconditioned matrix. The problems are investigated and a more general case is considered when the (1, 1) matrix block is close to the SLOUGH portfolio structure with such a matrix using the extended Lagrangian method. The theorem on the distribution of the spectrum of the matrix \(B^{-1}(\omega_1, \omega_2) \) is proved. For the preconditioner \(B(\omega_1, \omega_2) \) there is a generalization \(B(\omega_1, \omega_2) \)(5) for the main tasks.

\section*{Preconditioning a SLOUGH with a saddle matrix}

Characteristic of the problem leading to the solution of SLOWS with a saddle matrix is the following quadratic programming problem: it is necessary to find the minimum \(J(u)\) on the statement \(J(u)=\frac{1}{2}u^*E u - u^*f\) in the presence of \(q\leq p\) linear constraints \(E u = g\):
\[
\begin{aligned}
\left( \begin{array}{cc}
M & E^* \\
E & 0 
\end{array} \right)
\left( \begin{array}{c}
u \\
\lambda
\end{array} \right)
= 
\left( \begin{array}{c}
f \\
g
\end{array} \right),
\end{aligned}
\]
where \(M = M^*= C_{\times}\) is a positive definite matrix, \(E\) ∈ \(C^{q\times p} \) is an arbitrary matrix of full rank, \(q\leq p\), \(u\) ∈ \(C^p\), \(g\) ∈\(C^q\). This problem corresponds to the Lagrange functional \(L(u, \lambda) = J(u) + \mu^*(E u - g) \), where \(\mu\) is a vector of Lagrangian multipliers. Note that the matrix of block-structured SLAU (7) is non-negative if and only if [10]:
\[
\begin{aligned}
\text{rank}(E^*) = q, \\
\text{ker}(E) \cap \text{ker}(M) = \{0\}.
\end{aligned}
\]

\text{Transform }(3) \text{ to an equivalent implicit SLA whose matrix has sectr, deja vu and right and left } [11]:
\begin{equation}
\begin{pmatrix}
M & E^* \\
-E & 0 \\
\end{pmatrix}
\begin{pmatrix}
u \\
\mu \\
\end{pmatrix}
=
\begin{pmatrix}
f \\
-g \\
\end{pmatrix}
. (8)
\end{equation}

\text{Consider the case when }(1,1) \text{ marginal block is semi-regular or degenerate. We will use the extended Lagrangian method, which consists in replacing }(8)\text{ with SLOUGH}
\begin{equation}
\Delta w =
\begin{pmatrix}
M & E^* \\
-E & 0 \\
\end{pmatrix}
\begin{pmatrix}
u \\
\mu \\
\end{pmatrix}
=
\begin{pmatrix}
f + \gamma E^*g \\
-g \\
\end{pmatrix}
= F, (9)
\end{equation}
\text{in which } M\text{is replaced by the matrix } M = M + \gamma E^*E, \text{being positive definite for all } \gamma >0, \text{if } M\text{ has full rank. Obviously, }(9) \text{ has the same solution as }(8). \text{The most effective choice } \gamma = ||M||_2/|| E||_2 [12]. \text{In this case, the conditionality number of both the }(1,1)\text{ block and the entire matrix of coefficients is the smallest.}
\text{Let's imagine the matrix } A \text{ of } (9), \text{similarly} (2), \text{ as the sum of its Hermitian and coermitian components:}

A = A_0 + A_1, \quad A_0 = \begin{pmatrix} M & 0 \\ 0 & E^* \end{pmatrix}, \quad A_1 = \begin{pmatrix} 0 & E^* \\ -E & 0 \end{pmatrix}.

\text{The Coermite matrix } A_1, \text{ in turn, is represented as:}
A_1 = K_L + K_U = \begin{pmatrix} 0 & 0 \\ -E & 0 \end{pmatrix} + \begin{pmatrix} 0 & E^* \\ 0 & 0 \end{pmatrix},

\text{where } 0 \text{ is a zero matrix of suitable dimension, } K_L\text{ and }K_U\text{ are strictly lower— and strictly upper-triangular matrices, respectively.}

\text{Define the matrix } B \text{ as follows: } B = \begin{pmatrix} B_1 & 0 \\ 0 & B_2 \end{pmatrix},
\text{and }B_2\text{ are pre—calculated matrices.}

\text{Method } GSTS [9]. \text{ Let the initial approximation in the problem be } w^{(0)} = \begin{pmatrix} u^{(0)} \\\mu^{(0)} \end{pmatrix}\in C^{p+q}, \text{ and search by step iterative parameter } t. \text{ For } k = 0,1,2, \ldots \text{ until the convergence of the sequence of regional approximations is reached } \widetide{w}^{(k)}, \text{execute }

\begin{align}
B_2u^{(k+1)} &= B_2u^{(k)} + t[u^{EB}_1(-f - Mu^{(k)} - E^*\mu^{(k)} + Eu^{(k)} - g], \\
B_1\mu^{(k+1)} &= B_1\mu^{(k)} - tMu^{(k+1)} + t[E^*u^{(k+1)} - \gamma\mu^{(k)} - u^{(k+1)}+f ],
\end{align}

\text{where } t\text{ and } \gamma\text{ are some parameters that are not equal to zero at the same time.}

A GSTS-like algorithm is defined as follows [9]:
B(u_1, u_2) = (Bc + u_1K_L)B_1^{-1}(Bc +u_2K_U)\quad(10)
or, in block form,
\begin{equation}
B(u_1, u_2) = \begin{pmatrix} B_1 & \omega_2E^* \\ -\omega_1E & B_2 - \omega_1\omega_2E^*E^{-1} \end{pmatrix},
\end{equation}
where \(u_1\) and \(u_2\) are non—negative parameters that are not equal to zero at the same time.

Take \(B_1 = M\) as a matrix. The phrase of choosing the matrix \(B_2\) will be considered later. Then
\begin{equation}
B(u_1, u_2) = \begin{pmatrix} M & \omega_2E^* \\ -\omega_1E & B_2 - \omega_1\omega_2E^*E^{-1} \end{pmatrix} = \begin{pmatrix} M & \omega_2E^* \\ -\omega_1E & B_2 - \omega_1\omega_2S \end{pmatrix},
\end{equation}
where \(S = E^*E^{-1}\) is a positive definite matrix for \(M\).

A preconditioned (mixed) block SLA has the form:
\[ B^{-1}(u_1, u_2)Aw = B^{-1}(u_1, u_2)F. \quad (11) \]
We investigate the convergence of the preconditioned matrix from (11).

Theorem 1. Let the matrix \(A\) and \(B(u_1, u_2) \) be defined as \((9)\) and \((10)\), respectively, and \(B_1 = M\). Let \(\lambda\in\sigma(B^{-1}(u_1, u_2)A) \). Then for each eigenvalue of the transformed matrix there is an expression
\[
\lambda = \frac{v - \omega \xi \pm \sqrt{v (v - \omega\xi^2) - 4\xi v}}{2v},
\]
where \( \xi =\frac{\xi_1}{\xi_2} \), \(v=\frac{E^*z_2}{z_1} \), and the parameter \( \omega= \omega_1\omega_2 - \omega_1 - \omega_2 \).

Proof. Let \(\lambda\in\sigma(B^{-1}(u_1, u_2)A)\) and \((y^*, z^*) \in \mathbb{C}^n+\mathbb{C}^n\) correspond to this eigenvector, where \(y\in\mathbb{C}^n\), \(z\in \mathbb{C}^n\). Then we have
\[
\begin{pmatrix}
M & E^* \\
-E & 0 \\
\end{pmatrix}
\begin{pmatrix}
y \\
z \\
\end{pmatrix}
= \lambda
\begin{pmatrix}
M & \omega_2E^* \\
-\omega_1E & B_2 - \omega_1\omega_2S \\
\end{pmatrix}
\begin{pmatrix}
y \\
z \\
\end{pmatrix},
\]
or, equivalently,
\[
\begin{cases}
(1 - \lambda)My = (\omega_2 - 1)E^*z, \\
(\lambda - 1)Ey = \lambda B_2z - \lambda\omega_2S\zeta.
\end{cases}
\quad (12)
\]
Let \(\lambda\neq 1\). Otherwise, if \(\lambda = 1\), equations (12) reduce to
\[
\begin{cases}
(\omega_2 - 1)E^*z = 0, \\
(\omega_1 - 1)Ey = B_2z - \omega_1\omega_2S\zeta.
\end{cases}
\quad(13)
\]
and there exists a nonzero vector \( (y^*, 0)\) from \(\mathbb{C}^n+\{0\}\) such that equations (13) are valid for \(y\) from \(\mathbb{C}^n \setminus \{0\}\). If \(\lambda\neq 1 \), then from the first equation (12) we have:

\[
y = \frac{\lambda \omega_2 - 1}{1 - \lambda} M^{-1}E^*z.
\]

Substituting this relation into the second equation in (12), we obtain:

\[
\left( \frac{(\lambda - 1)(\lambda \omega_2 - 1)}{1 - \lambda} + \lambda \omega_2 \right) S\zeta = \lambda B_2z,
\]

or

\[
\lambda (\lambda + 1) S\zeta = \lambda (1 - \lambda) B_2z,
\]
(14)

where \( \omega = \omega_2 - \omega_1 - \omega_2 \). For convenience, we introduce the notation \( \xi = \frac{\xi_1}{\xi_2} \) and \(v=\frac{E^*z_2}{z_1} \); then, after multiplying (14) from the left by \(z^*\) and dividing by \(z^*z\) of both parts of this expression, we get:

\[
\lambda (\lambda + 1) \xi = \lambda (1 - \lambda) \nu,
\]

that is, \(\lambda\) is the root of the quadratic equation of the lower:

\[
\nu \lambda^2 - (\nu - \omega \xi) \lambda + \xi = 0.
\]
(15)

The statement of Theorem 1 follows directly from (15). Note that when \(\omega = -1\), the roots of (15) are \(\lambda = 1\) and \(\lambda =\xi/\nu\). The practical conclusion that follows from this theorem is as follows. If \(\omega = -1\) and \(\xi = \nu\), then all eigenvalues of the preconditioned matrix \(B^{-1}(\omega_1, \omega_2)A\) are equal to 1, which means that the efficiency of the considered preconditioner matrix \(B_2\) should maximize the approximate complement of the Schur\(S\).


\end{document}